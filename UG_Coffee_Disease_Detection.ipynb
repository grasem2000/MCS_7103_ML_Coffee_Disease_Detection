{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr7iTPNkpGTK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Coffee Disease Detection using MobileNetV2 and TensorFlow\n",
        "---------------------------------------------------------\n",
        "Classifies coffee leaf images into Healthy, Rust, and Phoma.\n",
        "Implements transfer learning, data augmentation, reproducibility, and deployment readiness.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hkfteddktOWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coffee_disease_detection.py\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Reproducibility Settings\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Dataset Parameters\n",
        "# -----------------------------\n",
        "IMG_SIZE = (256, 256)\n",
        "BATCH_SIZE = 100\n",
        "DATA_DIR = 'dataset'  # Root directory containing train/val/test subfolders\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Data Loading\n",
        "# -----------------------------\n",
        "def get_datasets(data_dir, img_size, batch_size, seed):\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        os.path.join(data_dir, 'train'),\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=img_size,\n",
        "        shuffle=True,\n",
        "        seed=seed\n",
        "    )\n",
        "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        os.path.join(data_dir, 'val'),\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=img_size,\n",
        "        shuffle=False,\n",
        "        seed=seed\n",
        "    )\n",
        "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        os.path.join(data_dir, 'test'),\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=img_size,\n",
        "        shuffle=False,\n",
        "        seed=seed\n",
        "    )\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "train_ds, val_ds, test_ds = get_datasets(DATA_DIR, IMG_SIZE, BATCH_SIZE, SEED)\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Data Augmentation\n",
        "# -----------------------------\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip('horizontal_and_vertical', seed=SEED),\n",
        "    layers.RandomRotation(0.1, seed=SEED),\n",
        "    layers.RandomZoom(0.1, seed=SEED),\n",
        "    layers.RandomContrast(0.1, seed=SEED),\n",
        "    layers.RandomBrightness(0.1, seed=SEED)\n",
        "], name='data_augmentation')\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Preprocessing Pipeline\n",
        "# -----------------------------\n",
        "def preprocess(image, label):\n",
        "    image = preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Model Building\n",
        "# -----------------------------\n",
        "def build_model(input_shape, num_classes, base_trainable=False, fine_tune_at=None):\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    base_model.trainable = base_trainable\n",
        "    if fine_tune_at is not None:\n",
        "        for layer in base_model.layers[:fine_tune_at]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape=IMG_SIZE + (3,), num_classes=len(class_names), base_trainable=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Compile Model\n",
        "# -----------------------------\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Compute Class Weights (Optional)\n",
        "# -----------------------------\n",
        "def get_class_weights(dataset):\n",
        "    labels = np.concatenate([y for x, y in dataset], axis=0)\n",
        "    y_integers = np.argmax(labels, axis=1)\n",
        "    class_weights = compute_class_weight('balanced', classes=np.arange(len(class_names)), y=y_integers)\n",
        "    return dict(enumerate(class_weights))\n",
        "\n",
        "class_weight = get_class_weights(train_ds)\n",
        "\n",
        "# -----------------------------\n",
        "# 9. Callbacks\n",
        "# -----------------------------\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "    'best_model.h5', monitor='val_loss', save_best_only=True, verbose=1\n",
        ")\n",
        "earlystop_cb = callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True, verbose=1\n",
        ")\n",
        "tensorboard_cb = callbacks.TensorBoard(log_dir='logs')\n",
        "\n",
        "callback_list = [checkpoint_cb, earlystop_cb, tensorboard_cb]\n",
        "\n",
        "# -----------------------------\n",
        "# 10. Initial Training (Feature Extraction)\n",
        "# -----------------------------\n",
        "EPOCHS = 30\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callback_list,\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 11. Fine-Tuning\n",
        "# -----------------------------\n",
        "# Unfreeze last 20 layers for fine-tuning\n",
        "model = build_model(input_shape=IMG_SIZE + (3,), num_classes=len(class_names), base_trainable=True, fine_tune_at=-20)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "# Load best weights from initial training\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "FINE_TUNE_EPOCHS = 10\n",
        "total_epochs = EPOCHS + FINE_TUNE_EPOCHS\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=history.epoch[-1] + 1,\n",
        "    callbacks=callback_list,\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 12. Evaluation\n",
        "# -----------------------------\n",
        "# Load best weights\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Classification report and confusion matrix\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "y_pred = model.predict(test_ds)\n",
        "y_true_labels = np.argmax(y_true, axis=1)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true_labels, y_pred_labels))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=class_names))\n",
        "\n",
        "# -----------------------------\n",
        "# 13. Save Model for Deployment\n",
        "# -----------------------------\n",
        "model.save('saved_model/coffee_disease_detector')\n",
        "# Optionally, save as H5\n",
        "model.save('coffee_disease_detector.h5')\n",
        "\n",
        "# Convert to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/coffee_disease_detector')\n",
        "tflite_model = converter.convert()\n",
        "with open('coffee_disease_detector.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# -----------------------------\n",
        "# 14. Grad-CAM for Interpretability (Example)\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# Usage example:\n",
        "# img = tf.keras.utils.load_img('sample.jpg', target_size=IMG_SIZE)\n",
        "# img_array = tf.keras.utils.img_to_array(img)\n",
        "# img_array = np.expand_dims(img_array, axis=0)\n",
        "# img_array = preprocess_input(img_array)\n",
        "# heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='Conv_1')"
      ],
      "metadata": {
        "id": "AWe_wwQctPNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5460ccfb-6e5c-4f99-d1bf-8570b7a9117c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3144442533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ]
    }
  ]
}